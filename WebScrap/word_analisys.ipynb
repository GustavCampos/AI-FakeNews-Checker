{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gustavo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Downloading NLTK data to use later\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing initial web scrapping result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Colors of Water Bottle Caps Have Hidden Meanings?   \n",
      "\n",
      "                                              Byline  \n",
      "0  According to some social media posts, black, b...  \n"
     ]
    }
   ],
   "source": [
    "LOCATION = os.getcwd()\n",
    "ARTICLES_CSV = os.path.join(LOCATION, \"articles.csv\")\n",
    "SCRAP_TOPICS_DIR = os.path.join(LOCATION, \"scrap_topics\")\n",
    "\n",
    "# articles.csv columns\n",
    "TITLE = 'Title'\n",
    "LINK = 'Link'\n",
    "BYLINE = 'Byline'\n",
    "DATE = 'Date'\n",
    "AUTHOR = 'Author'\n",
    "\n",
    "\n",
    "raw_csv_df = pd.read_csv(ARTICLES_CSV, encoding='utf-8')\n",
    "    \n",
    "title_byline_df = raw_csv_df[[TITLE, BYLINE]].copy(deep=True)\n",
    "\n",
    "print(title_byline_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found on scrap: 32964\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Word      |   Count |\n",
       "|:----------|--------:|\n",
       "| trump     |    2304 |\n",
       "| us        |    1710 |\n",
       "| president |    1617 |\n",
       "| show      |    1421 |\n",
       "| video     |    1385 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    return (\"\".join(ch for ch in text if ch.isalnum() or ch.isspace()).lower())\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "word_list = []\n",
    "for index, row in title_byline_df.iterrows():\n",
    "    if not pd.isnull(row[TITLE]):\n",
    "        formatted_title = normalize_text(row[TITLE])\n",
    "        word_list.extend(filter(\n",
    "            lambda word: word not in stop_words,\n",
    "            formatted_title.split()\n",
    "        ))\n",
    "\n",
    "    if not pd.isnull(row[BYLINE]):\n",
    "        formatted_byline = normalize_text(row[BYLINE])\n",
    "        word_list.extend(filter(\n",
    "            lambda word: word not in stop_words,\n",
    "            formatted_byline.split()\n",
    "        ))\n",
    "            \n",
    "word_df = pd.DataFrame(word_list, columns=['Word'])\n",
    "word_agg_df = word_df.groupby('Word').size().reset_index(name='Count')\n",
    "word_agg_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "\n",
    "print(f\"Words found on scrap: {len(word_agg_df)}\")\n",
    "display(Markdown(word_agg_df.head(5).to_markdown(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process meaning words and term frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>007themed</th>\n",
       "      <th>02</th>\n",
       "      <th>0233</th>\n",
       "      <th>030725</th>\n",
       "      <th>045</th>\n",
       "      <th>05</th>\n",
       "      <th>050</th>\n",
       "      <th>07</th>\n",
       "      <th>...</th>\n",
       "      <th>zuccotti</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerbergs</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulican</th>\n",
       "      <th>zunzuncito</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zxt</th>\n",
       "      <th>álvaro</th>\n",
       "      <th>širokibrijeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32959</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32960</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32961</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32962</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32963</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32964 rows × 32934 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  007  007themed   02  0233  030725  045   05  050   07  ...  \\\n",
       "0      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...    ...  ...        ...  ...   ...     ...  ...  ...  ...  ...  ...   \n",
       "32959  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32960  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32961  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32962  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32963  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "       zuccotti  zuckerberg  zuckerbergs  zuckerman  zulican  zunzuncito  \\\n",
       "0           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "1           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "2           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "3           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "4           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "...         ...         ...          ...        ...      ...         ...   \n",
       "32959       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32960       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32961       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32962       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32963       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "\n",
       "       zurich  zxt  álvaro  širokibrijeg  \n",
       "0         0.0  0.0     0.0           0.0  \n",
       "1         0.0  0.0     0.0           0.0  \n",
       "2         0.0  0.0     0.0           0.0  \n",
       "3         0.0  0.0     0.0           0.0  \n",
       "4         0.0  0.0     0.0           0.0  \n",
       "...       ...  ...     ...           ...  \n",
       "32959     0.0  0.0     0.0           0.0  \n",
       "32960     0.0  0.0     0.0           0.0  \n",
       "32961     0.0  0.0     0.0           0.0  \n",
       "32962     0.0  0.0     0.0           0.0  \n",
       "32963     0.0  0.0     0.0           1.0  \n",
       "\n",
       "[32964 rows x 32934 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(word_agg_df[\"Word\"])\n",
    "\n",
    "# Convert the TF-IDF matrix to a dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF dataframe\n",
    "display(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random state: 78\n",
      "Topic 0: ['policewoman', 'gwu', 'czar', 'whitlock', 'excluded', 'pitchman', 'spearheaded', 'coron']\n",
      "Topic 1: ['hacking', 'heartwormpreventative', 'coauthored', 'hopef', 'internetdistributed', '41lb', 'intruder', 'banditry']\n",
      "Topic 2: ['charter', 'huffington', 'racebased', 'questi', 'ownin', 'crosseyed', 'jeremiah', 'branded']\n",
      "Topic 3: ['planters', 'ocean', 'lauryl', 'engineers', 'lilly', 'pictur', 'jim', 'electrocute']\n",
      "Topic 4: ['messed', 'opcitation', 'desiderata', 'nprs', 'spires', 'brigham', 'enlistments', '580']\n"
     ]
    }
   ],
   "source": [
    "random_state = 78 #randint(0, 100)\n",
    "number_of_topics = 5\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LatentDirichletAllocation(n_components=number_of_topics, random_state=random_state)\n",
    "\n",
    "# Fit LDA model to the TF-IDF matrix\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Get the words associated with each topic\n",
    "n_top_words = 8\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "topics_dict= {}\n",
    "for topic_index, topic in enumerate(lda.components_):\n",
    "    topics_dict[topic_index] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        \n",
    "print(f\"Using random state: {random_state}\")\n",
    "for topic_index, topic_words in topics_dict.items():\n",
    "    print(f\"Topic {topic_index}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new CSVs with topic words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_article(word_list: list, text_to_check: str) -> bool:\n",
    "    if pd.isnull(text_to_check): return False\n",
    "    \n",
    "    norm_text = normalize_text(text_to_check)\n",
    "    \n",
    "    return reduce(\n",
    "        lambda acc, word: acc or (word in norm_text),\n",
    "        word_list,\n",
    "        False\n",
    "    )\n",
    "\n",
    "if not os.path.exists(SCRAP_TOPICS_DIR):\n",
    "    os.makedirs(SCRAP_TOPICS_DIR)\n",
    "    \n",
    "for topic_index, topic_words in topics_dict.items():\n",
    "    csv_dict = []\n",
    "    \n",
    "    for index, row in raw_csv_df.iterrows():\n",
    "        if insert_article(topic_words, row[TITLE]) or insert_article(topic_words, row[BYLINE]):\n",
    "            csv_dict.append({\n",
    "                TITLE: row[TITLE],\n",
    "                BYLINE: row[BYLINE],\n",
    "                LINK: row[LINK],\n",
    "                DATE: row[DATE],\n",
    "                AUTHOR: row[AUTHOR]\n",
    "            })\n",
    "            \n",
    "    pd.DataFrame(csv_dict).to_csv(\n",
    "        os.path.join(SCRAP_TOPICS_DIR, f\"scrap_topic_{topic_index}.csv\"), \n",
    "        encoding='utf-8', \n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
