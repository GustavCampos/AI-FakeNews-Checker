{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gustavo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Downloading NLTK data to use later\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing initial web scrapping result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Elon Musk Launching Cellphone To Compete With ...   \n",
      "\n",
      "                                              Byline  \n",
      "0  Rumor has it the \"Tesla Phone\" will have seaml...  \n"
     ]
    }
   ],
   "source": [
    "LOCATION = os.getcwd()\n",
    "ARTICLES_CSV = os.path.join(LOCATION, \"articles.csv\")\n",
    "SCRAP_TOPICS_DIR = os.path.join(LOCATION, \"scrap_topics\")\n",
    "\n",
    "# articles.csv columns\n",
    "TITLE = 'Title'\n",
    "LINK = 'Link'\n",
    "BYLINE = 'Byline'\n",
    "DATE = 'Date'\n",
    "AUTHOR = 'Author'\n",
    "\n",
    "\n",
    "raw_csv_df = pd.read_csv(ARTICLES_CSV, encoding='utf-8')\n",
    "    \n",
    "title_byline_df = raw_csv_df[[TITLE, BYLINE]].copy(deep=True)\n",
    "\n",
    "print(title_byline_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found on scrap: 32989\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Word      |   Count |\n",
       "|:----------|--------:|\n",
       "| trump     |    2309 |\n",
       "| us        |    1711 |\n",
       "| president |    1620 |\n",
       "| show      |    1423 |\n",
       "| video     |    1395 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    return (\"\".join(ch for ch in text if ch.isalnum() or ch.isspace()).lower())\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "word_list = []\n",
    "for index, row in title_byline_df.iterrows():\n",
    "    if not pd.isnull(row[TITLE]):\n",
    "        formatted_title = normalize_text(row[TITLE])\n",
    "        word_list.extend(filter(\n",
    "            lambda word: word not in stop_words,\n",
    "            formatted_title.split()\n",
    "        ))\n",
    "\n",
    "    if not pd.isnull(row[BYLINE]):\n",
    "        formatted_byline = normalize_text(row[BYLINE])\n",
    "        word_list.extend(filter(\n",
    "            lambda word: word not in stop_words,\n",
    "            formatted_byline.split()\n",
    "        ))\n",
    "            \n",
    "word_df = pd.DataFrame(word_list, columns=['Word'])\n",
    "word_agg_df = word_df.groupby('Word').size().reset_index(name='Count')\n",
    "word_agg_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "\n",
    "print(f\"Words found on scrap: {len(word_agg_df)}\")\n",
    "display(Markdown(word_agg_df.head(5).to_markdown(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process meaning words and term frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>007themed</th>\n",
       "      <th>02</th>\n",
       "      <th>0233</th>\n",
       "      <th>030725</th>\n",
       "      <th>045</th>\n",
       "      <th>05</th>\n",
       "      <th>050</th>\n",
       "      <th>07</th>\n",
       "      <th>...</th>\n",
       "      <th>zuccotti</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerbergs</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulican</th>\n",
       "      <th>zunzuncito</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zxt</th>\n",
       "      <th>álvaro</th>\n",
       "      <th>širokibrijeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32984</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32987</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32988</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32989 rows × 32959 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  007  007themed   02  0233  030725  045   05  050   07  ...  \\\n",
       "0      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...    ...  ...        ...  ...   ...     ...  ...  ...  ...  ...  ...   \n",
       "32984  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32985  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32986  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32987  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32988  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "       zuccotti  zuckerberg  zuckerbergs  zuckerman  zulican  zunzuncito  \\\n",
       "0           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "1           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "2           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "3           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "4           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "...         ...         ...          ...        ...      ...         ...   \n",
       "32984       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32985       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32986       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32987       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32988       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "\n",
       "       zurich  zxt  álvaro  širokibrijeg  \n",
       "0         0.0  0.0     0.0           0.0  \n",
       "1         0.0  0.0     0.0           0.0  \n",
       "2         0.0  0.0     0.0           0.0  \n",
       "3         0.0  0.0     0.0           0.0  \n",
       "4         0.0  0.0     0.0           0.0  \n",
       "...       ...  ...     ...           ...  \n",
       "32984     0.0  0.0     0.0           0.0  \n",
       "32985     0.0  0.0     0.0           0.0  \n",
       "32986     0.0  0.0     0.0           0.0  \n",
       "32987     0.0  0.0     0.0           0.0  \n",
       "32988     0.0  0.0     0.0           1.0  \n",
       "\n",
       "[32989 rows x 32959 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(word_agg_df[\"Word\"])\n",
    "\n",
    "# Convert the TF-IDF matrix to a dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF dataframe\n",
    "display(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random state: 36\n",
      "Topic 0: ['diego', 'hermine', 'selfmutilation', 'ateba', 'aluminum']\n",
      "Topic 1: ['hitmen', 'interact', 'interior', 'mosaics', 'sderot']\n",
      "Topic 2: ['jilted', 'getrank', 'biafran', 'jewelers', 'twas']\n",
      "Topic 3: ['outback', 'simpson', 'fill', 'postwar', 'filtration']\n",
      "Topic 4: ['kawaguchi', 'inconsolable', 'incontinent', 'increased', 'terrorists']\n",
      "Topic 5: ['000', 'seldom', 'unwelcome', 'secre', 'attorneys']\n",
      "Topic 6: ['love', 'oktoberfest', 'basis', 'allmetal', 'allocate']\n",
      "Topic 7: ['širokibrijeg', 'crayons', 'usafa', 'lis', 'lipton']\n",
      "Topic 8: ['socia', 'dubiously', 'dramatically', 'dramatized', 'bounced']\n",
      "Topic 9: ['donkeys', 'servitude', 'ranted', 'touring', 'deleting']\n"
     ]
    }
   ],
   "source": [
    "random_state = randint(0, 100) # 79 68\n",
    "number_of_topics = 10\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LatentDirichletAllocation(n_components=number_of_topics, random_state=random_state)\n",
    "\n",
    "# Fit LDA model to the TF-IDF matrix\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Get the words associated with each topic\n",
    "n_top_words = 5\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "topics_dict= {}\n",
    "for topic_index, topic in enumerate(lda.components_):\n",
    "    topics_dict[topic_index] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        \n",
    "print(f\"Using random state: {random_state}\")\n",
    "for topic_index, topic_words in topics_dict.items():\n",
    "    print(f\"Topic {topic_index}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new CSVs with topic words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_article(word_list: list, text_to_check: str) -> bool:\n",
    "    if pd.isnull(text_to_check): return False\n",
    "    \n",
    "    norm_text = normalize_text(text_to_check)\n",
    "    \n",
    "    return reduce(\n",
    "        lambda acc, word: acc or (word in norm_text),\n",
    "        word_list,\n",
    "        False\n",
    "    )\n",
    "\n",
    "if not os.path.exists(SCRAP_TOPICS_DIR):\n",
    "    os.makedirs(SCRAP_TOPICS_DIR)\n",
    "    \n",
    "for topic_index, topic_words in topics_dict.items():\n",
    "    csv_dict = []\n",
    "    \n",
    "    for index, row in raw_csv_df.iterrows():\n",
    "        if insert_article(topic_words, row[TITLE]) or insert_article(topic_words, row[BYLINE]):\n",
    "            csv_dict.append({\n",
    "                TITLE: row[TITLE],\n",
    "                BYLINE: row[BYLINE],\n",
    "                LINK: row[LINK],\n",
    "                DATE: row[DATE],\n",
    "                AUTHOR: row[AUTHOR]\n",
    "            })\n",
    "            \n",
    "    pd.DataFrame(csv_dict).to_csv(\n",
    "        os.path.join(SCRAP_TOPICS_DIR, f\"scrap_topic_{topic_index}.csv\"), \n",
    "        encoding='utf-8', \n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CSVs with custom topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [\n",
    "    \"openai\",\n",
    "    \"photograph\",\n",
    "    \"media\",\n",
    "    \"viral\",\n",
    "    \"image\",\n",
    "    \"photo\",\n",
    "    \"online\",\n",
    "    \"facebook\",\n",
    "    \"tweet\",\n",
    "    \"post\",\n",
    "    \"photographs\",\n",
    "    \"meme\",\n",
    "    \"twitter\",\n",
    "    \"picture\",\n",
    "    \"scam\",\n",
    "    \"content\",\n",
    "    \"internet\",\n",
    "]\n",
    "\n",
    "for index, row in raw_csv_df.iterrows():\n",
    "    if insert_article(word_list, row[TITLE]) or insert_article(word_list, row[BYLINE]):\n",
    "        csv_dict.append({\n",
    "            TITLE: row[TITLE],\n",
    "            BYLINE: row[BYLINE],\n",
    "            LINK: row[LINK],\n",
    "            DATE: row[DATE],\n",
    "            AUTHOR: row[AUTHOR]\n",
    "        })\n",
    "\n",
    "pd.DataFrame(csv_dict).to_csv(\n",
    "    os.path.join(SCRAP_TOPICS_DIR, \"custom_scrap_topic.csv\"), \n",
    "    encoding='utf-8', \n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
