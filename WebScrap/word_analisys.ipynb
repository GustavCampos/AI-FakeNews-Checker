{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gustav-\n",
      "[nltk_data]     campos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from IPython.display import display, Markdown\n",
    "from random import randint\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Downloading NLTK data to use later\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing initial web scrapping result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21496\n",
      "                                               Title  \\\n",
      "0  Elon Musk Launching Cellphone To Compete With ...   \n",
      "\n",
      "                                              Byline  \n",
      "0  Rumor has it the \"Tesla Phone\" will have seaml...  \n"
     ]
    }
   ],
   "source": [
    "LOCATION = os.getcwd()\n",
    "ARTICLES_CSV = os.path.join(LOCATION, \"articles.csv\")\n",
    "SCRAP_TOPICS_DIR = os.path.join(LOCATION, \"scrap_topics\")\n",
    "\n",
    "# articles.csv columns\n",
    "TITLE = 'Title'\n",
    "LINK = 'Link'\n",
    "BYLINE = 'Byline'\n",
    "DATE = 'Date'\n",
    "AUTHOR = 'Author'\n",
    "\n",
    "\n",
    "raw_csv_df = pd.read_csv(ARTICLES_CSV, encoding='utf-8')\n",
    "    \n",
    "title_byline_df = raw_csv_df[[TITLE, BYLINE]].copy(deep=True)\n",
    "\n",
    "print(len(title_byline_df))\n",
    "print(title_byline_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found on scrap: 32989\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Word      |   Count |\n",
       "|:----------|--------:|\n",
       "| trump     |    2309 |\n",
       "| us        |    1711 |\n",
       "| president |    1620 |\n",
       "| show      |    1423 |\n",
       "| video     |    1395 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    return (\"\".join(ch for ch in text if ch.isalnum() or ch.isspace()).lower())\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "word_list = []\n",
    "for index, row in title_byline_df.iterrows():\n",
    "    if not pd.isnull(row[TITLE]):\n",
    "        formatted_title = normalize_text(row[TITLE])\n",
    "        word_list.extend(filter(\n",
    "            lambda word: word not in stop_words,\n",
    "            formatted_title.split()\n",
    "        ))\n",
    "\n",
    "    if not pd.isnull(row[BYLINE]):\n",
    "        formatted_byline = normalize_text(row[BYLINE])\n",
    "        word_list.extend(filter(\n",
    "            lambda word: word not in stop_words,\n",
    "            formatted_byline.split()\n",
    "        ))\n",
    "            \n",
    "word_df = pd.DataFrame(word_list, columns=['Word'])\n",
    "word_agg_df = word_df.groupby('Word').size().reset_index(name='Count')\n",
    "word_agg_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "\n",
    "print(f\"Words found on scrap: {len(word_agg_df)}\")\n",
    "display(Markdown(word_agg_df.head(5).to_markdown(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process meaning words and term frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>007themed</th>\n",
       "      <th>02</th>\n",
       "      <th>0233</th>\n",
       "      <th>030725</th>\n",
       "      <th>045</th>\n",
       "      <th>05</th>\n",
       "      <th>050</th>\n",
       "      <th>07</th>\n",
       "      <th>...</th>\n",
       "      <th>zuccotti</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuckerbergs</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulican</th>\n",
       "      <th>zunzuncito</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zxt</th>\n",
       "      <th>álvaro</th>\n",
       "      <th>širokibrijeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32984</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32987</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32988</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32989 rows × 32959 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  007  007themed   02  0233  030725  045   05  050   07  ...  \\\n",
       "0      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4      0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...    ...  ...        ...  ...   ...     ...  ...  ...  ...  ...  ...   \n",
       "32984  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32985  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32986  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32987  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "32988  0.0  0.0        0.0  0.0   0.0     0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "       zuccotti  zuckerberg  zuckerbergs  zuckerman  zulican  zunzuncito  \\\n",
       "0           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "1           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "2           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "3           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "4           0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "...         ...         ...          ...        ...      ...         ...   \n",
       "32984       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32985       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32986       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32987       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "32988       0.0         0.0          0.0        0.0      0.0         0.0   \n",
       "\n",
       "       zurich  zxt  álvaro  širokibrijeg  \n",
       "0         0.0  0.0     0.0           0.0  \n",
       "1         0.0  0.0     0.0           0.0  \n",
       "2         0.0  0.0     0.0           0.0  \n",
       "3         0.0  0.0     0.0           0.0  \n",
       "4         0.0  0.0     0.0           0.0  \n",
       "...       ...  ...     ...           ...  \n",
       "32984     0.0  0.0     0.0           0.0  \n",
       "32985     0.0  0.0     0.0           0.0  \n",
       "32986     0.0  0.0     0.0           0.0  \n",
       "32987     0.0  0.0     0.0           0.0  \n",
       "32988     0.0  0.0     0.0           0.0  \n",
       "\n",
       "[32989 rows x 32959 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(word_agg_df[\"Word\"])\n",
    "\n",
    "# Convert the TF-IDF matrix to a dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF dataframe\n",
    "display(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random state: 49\n",
      "Topic 0: ['allergies', 'impregnating', 'correspondents', 'distributors', 'giza']\n",
      "Topic 1: ['morey', 'monumental', 'zoomedin', 'zombie', 'zuccotti']\n",
      "Topic 2: ['swishing', 'mckellen', 'kudrow', 'occur', 'commonly']\n",
      "Topic 3: ['dishwashing', 'wassermanschultzs', 'guggenheim', 'gull', 'mouths']\n",
      "Topic 4: ['tend', 'diatribe', 'pres', 'duke', 'crosser']\n",
      "Topic 5: ['grotesque', 'dan', 'naples', 'missler', 'replaces']\n",
      "Topic 6: ['moth', 'expense', '133', 'exempti', 'robberies']\n",
      "Topic 7: ['potassium', 'highvoltage', 'bags', 'baghdad', 'thenpresidential']\n",
      "Topic 8: ['bffs', 'dimitri', 'qu', 'nessun', 'nervous']\n",
      "Topic 9: ['traditions', '412pound', 'weiner', 'weight', 'native']\n"
     ]
    }
   ],
   "source": [
    "random_state = randint(0, 100) # 79 68\n",
    "number_of_topics = 10\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LatentDirichletAllocation(n_components=number_of_topics, random_state=random_state)\n",
    "\n",
    "# Fit LDA model to the TF-IDF matrix\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Get the words associated with each topic\n",
    "n_top_words = 5\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "topics_dict= {}\n",
    "for topic_index, topic in enumerate(lda.components_):\n",
    "    topics_dict[topic_index] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        \n",
    "print(f\"Using random state: {random_state}\")\n",
    "for topic_index, topic_words in topics_dict.items():\n",
    "    print(f\"Topic {topic_index}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new CSVs with topic words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_article(word_list: list, text_to_check: str) -> bool:\n",
    "    if pd.isnull(text_to_check): return False\n",
    "    \n",
    "    norm_text = normalize_text(text_to_check)\n",
    "    \n",
    "    return reduce(\n",
    "        lambda acc, word: acc or (word in norm_text),\n",
    "        word_list,\n",
    "        False\n",
    "    )\n",
    "\n",
    "if not os.path.exists(SCRAP_TOPICS_DIR):\n",
    "    os.makedirs(SCRAP_TOPICS_DIR)\n",
    "    \n",
    "for topic_index, topic_words in topics_dict.items():\n",
    "    csv_dict = []\n",
    "    \n",
    "    for index, row in raw_csv_df.iterrows():\n",
    "        if insert_article(topic_words, row[TITLE]) or insert_article(topic_words, row[BYLINE]):\n",
    "            csv_dict.append({\n",
    "                TITLE: row[TITLE],\n",
    "                BYLINE: row[BYLINE],\n",
    "                LINK: row[LINK],\n",
    "                DATE: row[DATE],\n",
    "                AUTHOR: row[AUTHOR]\n",
    "            })\n",
    "            \n",
    "    pd.DataFrame(csv_dict).to_csv(\n",
    "        os.path.join(SCRAP_TOPICS_DIR, f\"scrap_topic_{topic_index}.csv\"), \n",
    "        encoding='utf-8', \n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CSVs with custom topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6508 articles selected\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | Title                                                                              | Byline                                                                                               | Link                                                                  | Date          | Author            |\n",
       "|---:|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:--------------|:------------------|\n",
       "|  0 | AOC Really Said 'Ceasefire Means Someone Sees a Fire' in Viral Video?              | Alexandria Ocasio-Cortez was discussing a ceasefire in the Israel-Gaza War in a video made back  ... | https://www.snopes.com/fact-check/alexandria-ocasio-cortez-ceasefire/ | June 11, 2024 | Nick Hardinges    |\n",
       "|  1 | IRS Issuing $8,700 Stimulus Checks to Qualifying Americans in 2024?                | Online users looked for answers in June 2024 after noticing Google's trending searches displayed ... | https://www.snopes.com/fact-check/irs-8700-stimulus-check/            | June 11, 2024 | Jordan Liles      |\n",
       "|  2 | Steelers Coach Mike Tomlin 'Directed' Team Not to Participate in Pride Month?      | The claim went viral on X, with many commenters believing it was real.                               | https://www.snopes.com/fact-check/steelers-tomlin-pride/              | June 10, 2024 | Jack Izzo         |\n",
       "|  3 | New York's St. John the Divine Cathedral Lit Up in Rainbow Colors for Pride Month? | The images drew ire from conservatives online.                                                       | https://www.snopes.com/fact-check/st-john-divine-cathedral-pride/     | June 10, 2024 | Anna Rascouët-Paz |\n",
       "|  4 | Real Giant Octopus Photographed on Indonesian Coast?                               | Image shows local beachgoers standing around eerily large octopus.                                   | https://www.snopes.com/fact-check/giant-octopus-indonesian-coast/     | June 10, 2024 | Nur Ibrahim       |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_list = [\n",
    "    \"openai\",\n",
    "    \"photograph\",\n",
    "    \"media\",\n",
    "    \"viral\",\n",
    "    \"image\",\n",
    "    \"photo\",\n",
    "    \"online\",\n",
    "    \"facebook\",\n",
    "    \"tweet\",\n",
    "    \"post\",\n",
    "    \"photographs\",\n",
    "    \"meme\",\n",
    "    \"twitter\",\n",
    "    \"picture\",\n",
    "    \"scam\",\n",
    "    \"content\",\n",
    "    \"internet\",\n",
    "]\n",
    "\n",
    "csv_dict = []\n",
    "for index, row in raw_csv_df.iterrows():\n",
    "    if insert_article(word_list, row[TITLE]) or insert_article(word_list, row[BYLINE]):\n",
    "        csv_dict.append({\n",
    "            TITLE: row[TITLE],\n",
    "            BYLINE: row[BYLINE],\n",
    "            LINK: row[LINK],\n",
    "            DATE: row[DATE],\n",
    "            AUTHOR: row[AUTHOR]\n",
    "        })\n",
    "        \n",
    "export_df = pd.DataFrame(csv_dict)\n",
    "\n",
    "print(f\"{len(export_df)} articles selected\")\n",
    "display(Markdown(export_df.head(5).to_markdown()))\n",
    "\n",
    "export_df.to_csv(\n",
    "    os.path.join(SCRAP_TOPICS_DIR, \"custom_scrap_topic.csv\"), \n",
    "    encoding='utf-8', \n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
